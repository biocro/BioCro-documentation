[["index.html", "BioCro Development Introduction Note about the types of documentation in BioCro The online documentation Version Info", " BioCro Development Version info Introduction The documentation in this short book is aimed at developers—at those who will be writing new modules or modifying existing ones; and most especially at those fixing bugs in or adding new features to the BioCro simulation framework, and those involved with the overall maintenance of the BioCro package. Note about the types of documentation in BioCro There several categories of documentation for BioCro: The top-level README.md is for general information about BioCro and is intended for potential users of the package. The files in the man directory document the R functions and the data associated with the BioCro package. This documentation is intended for users of the package. The files in the vignettes directory generally contain long-form documentation, also intended for users of the package. There is documentation generated by doxygen from comments in the C++ source files. General users will be interested in the documentation of the BioCro module library modules. Developers will be interested in the implementation of the dynamical systems simulator. Finally, there are various .md and .Rmd files scattered throughout the package; these comprise the content of the Developer’s Manual (this manual, if you are reading this page as part of the bookdown book). This documentation is targeted at BioCro developers and maintainers. The online documentation All of the documentation mentioned above is available online at https://biocro.github.io. When online documentation is generated Documentation is automatically generated and published when a new release is created on GitHub. It is also generated (but not published) when a pull request is created or updated. In the latter case, the documentation is packaged as a Zip file available for download. To download a generated Zip file, go to the page for the Automatically generate documentation workflow and find a workflow run corresponding to the pull request whose documentation you wish to view. Click on it, and then find the artifact to download near the bottom of the page.1 In addition, one can manually trigger a workflow build of the documentation for a specific branch. To do this, go to the page for the Generate Documentation workflow and click on the Run workflow button. Choose which branch you want to document and which types of documentation to generate and then click the Run workflow button inside the dropdown window. (See the introductory section about types of documentation for discussion of the various types of BioCro documentation, and the following section for a discussion of how this documentation is packaged.) Once the manually-triggered workflow has finished running, click on the link for the workflow run and look for the artifact to download at the bottom of the page. The layout of the online documentation The layout of the online documentation is in the form of a pkgdown document. Each of the categories of documentation listed above corresponds to a portion of this document as follows: The README.md document appears on the pkgdown home page (BioCro in the menu bar). The man pages appear under the menu item References. (Note that although the page heading on the Reference page says Function reference, documentation of the various data sets is included as well.) The vignettes appear under the menu item Articles. The Doxygen documentation may be accessed from the menu bar item C++ Library. Note that there are various forms of this documentation of varying concision and comprehensiveness. The developer documentation (this book!) may be accessed by using the menu bar item Developer’s Manual. Note that the Developer’s Manual, the C++ documentation, and the PDF-style vignettes are not well integrated into pkgdown, and the user will have to click the browser back button to return to pkgdown proper to once again have access to the pkgdown menu items. In addition to the documentation proper, the menu bar contains some other useful links: The About page gives useful information about the documentation—namely, which branch and commit version it corresponds to and the commit and generation dates. The Changelog shows the BioCro version history. Finally, there is a search box at the right of the menu bar. Note that this will not search the portions of the documentation not integrated into pkgdown, that is, it will not search the Developer’s Manual, the C++ documentation, or the PDF-style vignettes. And the search box only functions when the pkgdown document is deployed on a server. Version Info This document was generated from the version of BioCro specified as follows: Commit Hash: b8ed268 Commit Date: Fri, 7 Mar 2025 13:33:02 -0600 Generation Time: Fri, 7 Mar 2025 22:14:51 +0000 If there is more than one run for a given pull request, you will probably want the latest one. If the Zip file has expired, you may have to manually call the workflow to regenerate it. Manually-triggered workflow runs may be found at https://github.com/biocro/biocro/actions/workflows/document.yml↩︎ "],["contributing-to-biocro.html", "1 Contributing to BioCro 1.1 Making Changes 1.2 Code style 1.3 Formatting Tools", " 1 Contributing to BioCro 1.1 Making Changes 1.1.1 Discuss first Check the list of GitHub issues for a discussion of the issue. If there is not one, create an issue with a description of the problem and your proposed solution. By making changes without discussing it with the group, you risk spending time working on a solution that others may not accept. The members of the group also have diverse backgrounds and likely can give valuable design insights. 1.1.2 Follow BioCro’s git branching structure In general, BioCro development follows the git-flow branching model, where there are two permanent branches (main and develop) and three types of temporary branches (hotfixes, features, and releases). For most contributors, it is only necessary to know that most changes should be accomplished through feature branches, which are branched off develop and merged back into develop via a pull request that must be approved by one or more other developers. The remainder of this section discusses our system in more detail. Beyond the basic description of git-flow, we have a few additional rules and clarifications specific to BioCro development: Any merge into main or develop must be done via a pull request. This requirement is enforced with GitHub branch protection rules and cannot be bypassed. All pull requests into main and develop require approval before merging. This requirement is not enforced using GitHub branch protection rules, but please do not unilaterally merge a pull request without any form of approval from another developer. (There are two exceptions to this rule related to hotfix and release branches, described below.) The BioCro R package and repository follow semantic versioning of the form major.minor.patch. Hotfix branches should typically only increment the third (patch) number. Most release branches will increment the second (minor) number, and very rarely the first (major) number. Feature branches do not directly change the version number. Hotfix and release branch names should be formatted like type-version, where type is either hotfix or release, and version is the new version number. For example, hotfix-v3.0.3 would be a hotfix branch that changes the version number to 3.0.3. Feature branch names should reflect their purpose, and can be anything other than master, develop, hotfix-*, or release-*. Whenever the package version changes, a description of the related changes should be added to the changelog in NEWS.md as a new section titled with the new version number. Such version changes will happen via release and hotfix branches. Release branches will generally include changes made during the course of several feature branches, so they may require a significant amount of documentation in NEWS.md. To make it easier to prepare a list of these changes, each feature branch should include a description of its own changes in an UNRELEASED section of NEWS.md. For more information about updating the changelog, please see the comment at the top of NEWS.md. The following is a short description of BioCro’s implementation of the git-flow branching model: The main branch always contains the latest stable release of the BioCro GitHub repository. In fact, a new tag and release is made any time changes are merged into main, and such changes should always be accompanied by an increment to the BioCro R package version number. The develop branch contains bug fixes and new features that have been completed but may not have been released yet. Hotfix branches are for urgent changes that warrant immediate incorporation into main; typically these are bug fixes. A hotfix branch should be branched off main. When ready, it should first merged into main via an approved pull request. Then, a second pull request should be made to merge into develop. If there are no merge conflicts or test failures, this second request can be merged without any additional approval. When both merges are complete, the hotfix branch should be deleted. Feature branches are for less urgent changes that do not require immediate release; for example, the addition of a new R function to the package namespace. A feature branch should be branched off develop and merged back into develop via an approved pull request. Before merging, NEWS.md should be updated with a description of the changes under the # UNRELEASED section (see NEWS.md for more details). When the merge into develop is complete, the feature branch should be deleted. When sufficiently many changes have accumulated in develop to justify a new version of the package, a release branch is used to move the changes from develop to main. Release branches should not include substantive changes; rather, a release branch is primarily used to increment the package version and to ensure an up-to-date changelog in NEWS.md; see the “New Release” chapter for more details. A release branch should be branched off develop. When it’s ready, it should first merged into develop via an approved pull request. Then, a second pull request should be made to merge into main. (If the first PR target is main, it will be difficult to discern the minor changes in the release branch from the other changes in develop.) If there are no merge conflicts or test failures, this second request can be merged without any additional approval. When both merges are complete, the release branch should be deleted. 1.1.3 Etiquette for pull requests We have several conventions when making and reviewing pull requests that are designed to make the process as efficient as possible, and to minimize any issues introduced by new code. These have been used in many PRs and have been found to be helpful. Single ownership of each PR: If multiple people are making changes to the same code, there is a potential for conflicting changes to occur. While git provides a way to resolve such conflicts, it is better to avoid causing them in the first place. A simple way to prevent most of these conflicts is to treat each branch and PR as having a single “owner” who is in charge of making all commits. There are a few consequences of this approach: Pull request reviewers should suggest changes, not commit changes directly to the PR branch. If a reviewers wishes to contribute specific code changes, it is better to make them in a new branch and create a secondary PR to merge the changes into the original PR. Pull request owners should either implement the suggested changes or give a short explanation for why they have chosen not to. Once one or more reviewers have indicated their approval, it is the responsibility of the pull request owner to finalize the PR by merging it and deleting the branch. Submitted code should work: It takes time and effort to produce a thoughtful review of a pull request, so requests to review a PR should generally only be made for working code. In a BioCro context, “working” code means that R CMD check does not produce any errors and that all the vignettes can be built. See the “Running R CMD check” chapter for more information. While this does not guarantee that the new code achieves its goals, is fully documented, or follows the coding style guidelines, it is nevertheless a minimum requirement for any code to be incorporated into BioCro. The following is a thorough workflow that should ensure submitted code is working: Run the regression tests locally. The tests don’t take much time to run, so this is a good way to get fast feedback. Instructions for running the regression tests locally can be found in the “Running the testthat Tests” chapter. When all the regression tests are passing, run R CMD check locally. This takes longer than just running the regression tests, but it will also make sure that all code in the examples (and some of the vignettes) is able to run, which sometimes turns up more issues. Instructions for running R CMD check locally can be found in “Running R CMD check locally” section. Note that R CMD check will stop checking the examples once it encounters an error, so its list of problematic examples is not exhaustive. If there is an issue with an example, fix it and rerun R CMD check again because there may be additional errors in the examples. When R CMD check is passing locally, build all the vignettes. Because of space limits imposed by CRAN, some of our vignettes are designated as “web only.” Such vignettes are stored in the vignettes/web_only directoty and are not checked by R CMD check. They can be built manually on a case-by-case basis from within R by using tools::build_vignette, or all at once by running pkgdown::build_site() in an R session whose working directory is set to the root directory of the BioCro repository. See the “Building package vignettes” section for more information. When R CMD check is passing locally and the vignettes can all be built locally, make a PR, but don’t assign any reviewers. Making the PR will cause the workflow tests to run, which includes running R CMD check on several operating systems and versions of R, and building all the vignettes as part of the package documentation. This step takes the most time, but is important because sometimes an issue will appear that didn’t occur locally on your own operating system or version of R. When the online checks have passed, then assign reviewers to begin the review process. Doing this will ensure that there are no basic issues when reviewers begin to look at the code. It may seem like a hassle, but it saves time in the long run. Even when the exact workflow above is not followed, the online tests can still be extremely helpful to a PR owner or reviewer: It is possible to see whether the checks have passed by looking at the list of current PRs, where a red X next to a PR name indicates failure and a green check mark indicates success. When the tests have failed, more details can be found in the “Checks” tab of the PR web site. This will include the exact error messages, which are the starting point for troubleshooting. Of course, the conventions above are not hard rules, and there may be situations where it is acceptable, or even necessary, to deviate from them. The following is a (non-exhaustive) list of situations where these conventions could be modified: A PR owner may give clear permission to another developer to make direct code changes. This may be in response to a reviewer asking to make changes. Sometimes the online tests will fail due to an issue with the testing setup itself. In that case it might not be necessary for them to pass. However, in this scenario, it is important for at least one developer to ensure that R CMD check passes locally. Although it is ideal to only submit working code, sometimes this is not possible for a variety of reasons. If you are working on new code and wish to discuss it but there are R CMD check errors, this can be accomplished via a PR, a draft PR, a GitHub issue, or a GitHub discussion. The best approach may vary with the particular situation. Regardless of which approach is taken, it is essential to acknowledge the R CMD check issue and be clear about the type of feedback or discussion you are requesting. Sometimes a working PR may begin failing R CMD check after the author has made code changes in response to reviewer feedback. This is normal; the new problems and their solutions will simply become part of the PR discussion. 1.1.4 Making large modifications to BioCro From time to time, someone will propose making a large change to the organizational structure of BioCro or to one of its central components. Here we also consider any modification that influences the way users or developers interact with BioCro to be “large.” Large changes must be carefully considered and discussed before they are implemented. When considering such proposals, a number of key points should be kept in mind: BioCro is designed for computational modelers who want to focus on biology rather than computer programming; it is essential for BioCro to be easy to install and use. A friendly user experience makes BioCro accessible to a wide range of users who each have the potential to contribute to broader scientific understanding through modeling. Thus, it is important to minimize any barriers that may prevent some scientists from using it. BioCro is developed and maintained by a small team, most of whom have numerous other responsibilities. Thus, it is important to carefully consider the implementation and maintenance costs that may be associated with any proposed change, carefully weighing these against the perceived benefits. Changes to BioCro R packages should be consistent with CRAN policies. Distributing BioCro via CRAN is a key part of keeping it accessible to all users with maximum ease. At its core, BioCro is a C++ library; the BioCro R package merely provides a convenient R interface. Even though this R interface exists, we wish the C++ core to remain usable on its own. This has two implications: The core C++ library should maintain a consistent interface. Any changes to the public API should be carefully considered and carefully delineated.2 It should be and should remain relatively painless to obtain and use this library without having to have a full R installation. With BioCro, a premium is put on keeping it relatively self-contained; needless dependencies should be avoided. There are a number of reasons for this: Limiting dependencies will make it less likely BioCro will break for reasons beyond our control. Limiting dependencies reduces security risks. (On this point see, for example, Russ Cox’s discussion of the 2017 Equifax fiasco in his article Our Software Dependency Problem.) Limiting dependencies makes reproducibilty easier. If one wishes to replicate results of a BioCro simulation, the task is more difficult if one has to worry not only about what platform, what version of R, and what version of BioCro were used to obtain the original results, but if, on top of that, one has to worry about the versions of other R packages depended upon.3 Limiting dependencies means that fewer steps are required to install BioCro. BioCro has few dependencies, and all things being equal, we would like to keep it that way.4 If you propose a large modification to BioCro, please be prepared to discuss the following questions: How will time costs change for maintainers, developers, and users? Will there be more or fewer opportunities for BioCro to break due to changes in its dependencies? Which BioCro features will be added or lost? 1.1.5 Releasing a new version of BioCro When releasing a new version of BioCro, it is essential to make sure that NEWS.md is up to date and that the new version is acceptable to CRAN. This can be achieved through the following steps: Choose the next version number; our conventions for semantic versioning are described in NEWS.md. Make a new release branch whose name is formatted as release-vX.Y.X, where X.Y.Z is the new version number, as described in the “Branching Structure” chapter. Update the DESCRIPTION file with the new release date (set to today’s date), the new version number, and any necessary changes to the author list. Change the UNRELEASED header in NEWS.md to the new version number, and check the contents of this section to ensure it is clear and complete. It may be helpful to look through the list of completed pull requests on GitHub to check for any important changes that may have been missed. Run R CMD check to see any NOTEs that are reported; see the “Running R CMD check chapter” for instructions. Check the contents of cran-comments.md to make sure it accurately reflects the R CMD check notes. Make a pull request, requesting to merge the new branch into develop. In general, follow the guidelines in the “Pull Request Etiquette” chapter. However, there is one extra requirement for a release branch: the branch should not be merged until the new version has been accepted by CRAN. Sometimes CRAN may have issues with a new release, and it is better to address them before finalizing the release. (Otherwise, there may be several releases in quick succession with only minor or trivial changes between them; for example, BioCro versions 3.1.1 and 3.1.2.) See the “Submitting to CRAN” chapter for instructions. When the new version is on CRAN, merge the release branch into develop and then main. 1.1.6 Submitting a new version of BioCro to CRAN The package maintainer is responsible for submitting to CRAN, and the process consists of the following steps: Build the package with R CMD build5 using the current release version. Test the resulting .tar.gz file using R CMD check --as-cran using the current release version and the current development version on Linux and Windows. GitHub actions run this for R current Windows and Linux, and R devel Linux, so only R devel Windows needs to be checked locally. Attach the resulting .tar.gz file to the form here: https://cran.r-project.org/submit.html. Paste the contents of cran-comments.md in the form’s comments box. Submit, cross fingers, and wait. If any issues are found by CRAN, address them and try again. If the checks fail only from permissible NOTES, such as using C++11, reply to the email indicating the justification, for example, we use a library that uses C++11. You can restate what is in cran-comments.md. BioCro’s online testing system should catch most issues before reaching this point, but sometimes CRAN starts enforcing rules that are not clearly explained anywhere or not checked by R CMD check. The most up-to-date description of CRAN’s requirements can be obtained from the following official and semi-official sources, which each offer a different perspective on CRAN submission: https://cran.r-project.org/web/packages/policies.html https://r-pkgs.org/release.html#release https://github.com/DavisVaughan/extrachecks 1.2 Code style (Most of what is discussed here pertains specifically to code for the BioCro C++ library.) 1.2.1 Scientific considerations 1.2.1.1 Document sources and justifications in the code Include citations to sources for equations and parameters used in the code. The citation should be sufficient to locate the article and relevant information within it. Include a table or figure reference if appropriate. Use Doxygen-style comment syntax for high-level documentation of functions and classes. We use the Javadoc style of comment block in our code. (See the documentation of the Solar Position module solar_position_michalsky for an example of a Doxygen-style comment, and then look at the way this is rendered in the generated documentation.) Include reasoning and justification for the model, including assumptions that determine when use of the model is appropriate. These descriptions should be succinct. 1.2.1.2 Document units in the code After every physical quantity, include a comment with the units. The idea is that every quantity will roughly be read as if it were written in normal text: for example, double yield = 10 // Mg / ha should be read as meaning “the yield was 10 Mg / ha”. Using dimensions instead of units is acceptable if the code is written with the expectation that coherent units are used. The following example shows how to indicate units in a number of different contexts. Note that, as in LaTeX, ^ is used to indicate a superscript, so that m^2 indicates square meters. // In function signatures double ball_berry(double assimilation, // mol / m^2 / s double atmospheric_co2_concentration, // mol / mol double atmospheric_relative_humidity, // Pa / Pa double beta0, // mol / m^2 / s double beta1) // dimensionless from [mol / m^2 / s] / [mol / m^2 / s] // In assignments double leaf_temperature = air_temperature - delta_t; // K. // In return statements return assimilation_rate; // micromoles / m^2 / s. // In tables const std::map&lt;SoilType, soilText_str&gt; soil_parameters = { // d = dimensionless // d d d J / kg d J s / m^3 d d d Mg / m^3 // silt clay sand air_entry b Ks satur fieldc wiltp bulk_density { SoilType::sand, { 0.05, 0.03, 0.92, -0.7, 1.7, 5.8e-3, 0.87, 0.09, 0.03, 1.60 } }, { SoilType::loamy_sand, { 0.12, 0.07, 0.81, -0.9, 2.1, 1.7e-3, 0.72, 0.13, 0.06, 1.55 } }, }; If you would like to include other details, include the units in the same way, and include details following the units so that the variables are still read like regular text. For example, write ✓ return gswmol * 1000; // mmol / m^2 / s. Convert from mol to mmol. not ✗ return gswmol * 1000; // Converting from mol / m^2 / s to mmol / m^2 / s. Note that in a case such as this, the units apply to the entire value (gswmol * 1000) and not merely to the variable (gswmol). Use SI conventions for units and dimensions, including capitalization. Specifically, use “degrees C”, not “C”, to indicate °C. Use full names when symbols are not available: micromoles / m^2, not umol / m^2 degrees C, not *C. Use dimensionless for dimensionless quantities, and include how the dimensions have canceled if that is informative. Use ^ to indicate exponentiation: m^2, not m2. Prefer an asterisk to indicate multiplication; but indicating multiplication by juxtaposing units with exactly one space between them is acceptable. Prefer exactly one space on each side of the asterisk: kg * m / s or kg m / s. Either a solidus (“/”) or negative exponent is acceptable to indicate division, but ensure that the solidus is used correctly if used multiple times. Prefer exactly one space on each side of the solidus. 1.2.1.3 Document parameters When adding models that require new parameters, document the parameters in the parameter table in src/parameters.h. Please keep the table well formatted. If you are working on a model with undocumented parameters, it would be nice if you added them to the table as you work through the issue. 1.2.2 General coding considerations Do not use C-style arrays. Use an appropriate data type from the standard library instead. Use cmath, not math.h, for common mathematical functions. Be careful with using-directives (e.g. using namespace std) in a global scope; do not use them in global scope in a header file. Try to make using-declarations (e.g. using std::string) as local as possible. Type aliases (e.g. using string_vector = std::vector&lt;std::string&gt;) are perfectly acceptable in the global scope of a header file. Strongly prefer the coherent set of SI units. Doing so reduces code complexity remarkably as no conversions are necessary. Yes, no one publishes values with these units, but do the conversion in one place, the manuscript, instead of dozens of times in the code, constantly having to look up units for variables, and then spending hours debugging silly, difficult-to-find errors. The coherent set of SI units consists of all the units without prefixes, except that kg is the coherent unit of mass, not g. Do not copy and paste code, changing only small parts. Choose a design that eliminates the duplication. Make an effort to write unit tests. (See the vignette An Introduction to BioCro for Those Who Want to Add Models for information about writing unit tests—specifically, writing unit tests for new modules.) Do not mix sweeping formatting changes with behavior changes. Large formatting changes should be a separate commit, containing only formatting changes, and the commit comment should indicate that only formatting was changed. This way, code that changes program behavior won’t be obscured by a mass of changes to the formatting of the code. The Standard C++ Foundation’s C++ Core Guidelines have useful advice about aspects of coding and design. 1.2.3 Formatting code (Again, except in a few instances, this pertains specifically to C++ code.) The most important aspect of formatting is that the code is easy to understand. Below are unenforced preferences. Prefer underscores_in_identifiers not CamelCaseInIdentifiers and, in R, not dots.in.identifiers. Prefer lowercase-only identifiers. An exception may be made for commonly-recognized names used in a small scope, for example, I = V / R; F = m * a; E = m * (c * c); Avoid unnecessary parentheses. For example, use “a * b / c” instead of “(a * b) / c” or “a * (b / c)”. But in cases where the order of operations affects the result, parentheses may be used to erase any doubt in the mind of the reader (or the programmer!) as to what that order is. Thus, writing (a / b) * c instead of (the equivalent) a / b * c is acceptable. Parentheses may also be used to group portions of a formula that are commonly considered as a sub-unit, where they provide some semantic value (see the previous bullet point). Consider naming parts of a complicated expression in order to break it down into simpler ones. For example, x = (-b + sqrt(b * b - 4 * a * c)) / (2 * a); may be rewritten in three lines as num = -b + sqrt(b * b - 4 * a * c); denom = 2 * a; x = num / denom; Note that in C++, unlike in R, return statements do not require parentheses around the returned expression. Restrict the line length of paragraph-like comments to 80 characters, excepting a compelling reason to do otherwise. Lines in sections that are not paragraph-like could be somewhat longer if it facilitates presenting material in a more readable format. In the following snippet from the module library documentation, for example, we have allowed slighly-longer lines in order to be able to maintain one line per interval: /* * However, this definition is flexible. For example, for our soybean model * (soybean_development_rate_calculator.h) we define the intervals as follows: * -1 &lt;= DVI &lt; 0 : Sowing to Emergence * 0 &lt;= DVI &lt; 1 : Emergence to R1 (Flowering) is broken into three stages. * 0 &lt;= DVI &lt; 0.333 : Emergence to V0 (Cotyledon stage) * 0.333 &lt;= DVI &lt; 0.667 : V0 (Cotyledon stage) to R0 (End of Floral Induction) * 0.667 &lt;= DVI &lt; 1 : R0 (End of Floral Induction) to R1 (Flowering) * 1 &lt;= DVI &lt; 2 : R1 (Flowering) to R7 (Maturity) */ As for the code lines themselves, we point to the following advice from the Linux kernel project:1 The preferred limit on the length of a single line is 80 columns. Statements longer than 80 columns should be broken into sensible chunks, unless exceeding 80 columns significantly increases readability and does not hide information. Do not include trailing whitespace, i.e., whitespace characters preceding newline characters. Each file should end with a newline character (i.e. a terminal endline). Use spaces rather than tab characters. In general, formatting preferences should follow something similar to the Google C++ style guide, except in cases where the code has been formatted in a more readable way, such as when aligning parts in a table. The Standard C++ Foundation guidelines offer some advice about formatting conventions that are informative, particularly regarding the use of code comments. For tools to help with formatting code, see the “Formatting Tools” section. 1.2.4 R-specific coding advice Prefer to use the double-bracket operator (list[['element']]) rather than the dollar-sign operator (list$element) when accessing the elements of a list. The $ operator uses partial matching, whereas [[, by default, does not. (However, it can be specified: list[['element', exact = FALSE]].) Avoiding partial matching by using [[ gives us more confidence that errors won’t occur. While there is no inherent performance difference between a for loop and an apply-type function such as apply or lapply (the apply functions actually use for loops in their source code), it is nevertheless possible to write a “bad” for loop that runs slowly. Common culprits include a failure to pre-allocate memory or a poor choice in assignment method. If a for loop seems to run slowly, consider replacing it with an apply-type function or tweaking the assignment method (e.g. replacing append with [). Many guides for optimizing loop performance are available online, such as Strategies to Speedup R Code and Why loops are slow in R. Generally our R code follows the Advanced R (1e) Style Guide with the exception of indentation, where we use 4 spaces rather than 2. Put a space after control statements (such as if or for), but do not put a space after other function names (such as function, return, or sqrt). For example: myfun &lt;- function(x) { y &lt;- if (x == 1) { 2 } else { sqrt(x) } return(y) } (Yes, in R, function is a function.) 1 The Linux kernel project recently changed the default length for code lines from 80 to 100 characters with the following commit comment: Yes, staying withing 80 columns is certainly still preferred. But it’s not the hard limit that the checkpatch warnings imply, and other concerns can most certainly dominate. Increase the default limit to 100 characters. Not because 100 characters is some hard limit either, but that’s certainly a “what are you doing” kind of value and less likely to be about the occasional slightly longer lines. ↩︎ 1.3 Formatting Tools 1.3.1 Clang’s formatting tool Many of these BioCro formatting preferences can be applied automatically using the program clang-format with the .clang-format file provided in the base directory of BioCro. Do not apply clang-format to all files indiscriminately, as that will ruin manually-aligned tables. The best time to reformat a file is immediately before (or possibly immediately after) making substantive changes to the code in the file. But, as mentioned in the “General Coding Considerations” section, sweeping formatting changes should be made in a separate commit, separate from any substantive changes made to the code. This way, changes to the functioning of the code won’t be obscured by changes to formatting that have no effect on code semantics. 1.3.1.1 Installation One can install clang-format on Ubuntu using sudo apt install clang-format. On macOS, clang-format is available from the Homebrew package manager. 1.3.1.2 Using the Clang formatting tool Files can be formatted using clang-format file_name &gt; new_file or edited in place using clang-format -i file_name If your editor has the ability to display differences between the original and revised versions of the file, it is a good idea to step through and inspect the proposed changes to ensure they are desirable. 1.3.1.3 Using the Clang formatting tool with the CodeLite IDE On Windows, macOS, or Linux, the CodeLite IDE includes clang-format and provides an easy way to use it. First go to Plugins -&gt; Source Code Formatter -&gt; Options. In the C++ tab, select use .clang-format file. Now press Ctrl-I or click Plugins -&gt; Source Code Formatter -&gt; Format Current Source to format a file. 1.3.2 EditorConfig Another tool to help with formatting is EditorConfig. EditorConfig, when used in conjunction with the .editorconfig file provided in the base directory of BioCro, provides a method for standardizing settings across different text editors. While some editors have native support, others require a plugin. See the EditorConfig website for more details. As of this writing, no C++ API for BioCro has been defined: there is no document that makes clear what publicly accessible portions of the framework and standard library are guaranteed to remain stable and available to be programmed against and which portions are subject to change.↩︎ Irrespective of what dependencies BioCro now has or are added to it, a researcher who is concerned about reproducability should consider making a containerized version of BioCro. See, for example, the chapter Docker and Reproducibility in the document for the workshop Reproducible analysis and Research Transparency. The BioCro maintainers don’t provide containerized versions of BioCro as we think this is a task better left to the individual researcher.↩︎ BioCro’s strong dependencies are the R framework, the C++ compiler used in installing the BioCro package, the C++ Standard Library, the and the Boost C++ Library. As for R package dependencies, the BioCro R package depends only upon packages in the R standard library (stats and utils) for its basic installation and functioning. BioCro does use other, non-standard R packages for building the documentation and for testing, but these are not essential to a fully-functioning BioCro installation. For further reading on the benefits and pitfalls of using dependencies, see, for example, Russ Cox’s article Our Software Dependency Problem and Bill Sourour’s article Code dependencies are the devil.↩︎ If a package contains large vignette files, R CMD check may produce an error about overly large documentation. In this case, it can be helpful to specify --compact-vignettes=both when calling R CMD build. Previously this was an important issue for BioCro, but it has been mitigated by designating most vignettes as “web only.”↩︎ "],["generating-documentation.html", "2 Generating Documentation 2.1 Online documentation 2.2 When to generate documentation 2.3 Documenting the C/C++ code. 2.4 Building package vignettes 2.5 Compiling and viewing the bookdown book", " 2 Generating Documentation 2.1 Online documentation Before we discuss generating documentation, we reiterate that you rarely need to. That is because the documentation is automatically generated for you and made available online! The public documentation page at https://biocro.github.io provides up-to-date documentation for the latest release of BioCro. (This documentation will provide links to documentation of older releases.) 2.2 When to generate documentation For the most part, developers should only need to generate documentation when they wish to check how changes made to it will be rendered, and whether that rendering is more or less correct. Changes a developer might make to the documentation include: Changes to Doxygen-style comments in the C++ source code files Changes to R function and data documentation (the files in the man directory) Changes to any of the Markdown (.md) or R Markdown (.Rmd) files included in the Bookdown book (this book). (A list of all files used in forming the Bookdown book is in the configuration file bookdown/_bookdown.yml.) Changes to the vignettes In most other cases, developers can simply consult the automatically-generated online version of the documentation. 2.3 Documenting the C/C++ code. To generate documentation for the C/C++ code, we use Doxygen. 2.3.1 Use cases for generating the Doxygen documentation As mentioned in the “When to Generate Documentation” section, developers should rarely need to generate documentation themselves since all of the documentation is regularly generated and published on-line. But there are cases where it makes sense to “do it yourself.” For Doxygen, these include: You are altering the code or documentation of a particular C++ source file, and you want to check that changes you make show up correctly in the generated documentation. You want a local copy of the Doxygen documentation because you don’t want to rely upon always having an internet connection. You are making large-scale changes to the code, and it would be useful to have a version of the documentation that tracks with your changes. You prefer a different presentation style for the documentation than is used in the on-line version. Use Cases 2 and 3 are best addressed using the simple methods of the “Running Doxygen” section. Use Case 1 is greatly facilitated by speeding up Doxygen using the methods discussed in the “Speeding up Doxygen” section. Lastly, Use Case 4, which requires a more extensive knowledge of Doxygen, is discussed in the “Customizing Doxygen” section. If none of these use cases apply to you, you most likely can simply use the on-line documentation (follow the C++ Library menu-bar link) and skip the rest of this chapter. 2.3.2 Required software Doxygen (version 1.9.2 or higher) The Graph visualization toolkit (“GraphViz”; version 2.38 or higher) You can get by without this if you don’t care about generating the dependency and inheritance graphs. If you don’t have GraphViz installed, set “HAVE_DOT = NO” in the customization file (see below). A LaTeX distribution This is needed only if you set USE_MATHJAX = NO, or if you want to generate the PDF version of the documentation. Ghostscript You may be able to get by without this. You will need it, however, if you wish to generate HTML documentation that doesn’t rely on MathJAX.6 Gnu Make (version 4.3 or higher)7 This is needed only if you want to take advantage of the Makefile recipes. 2.3.3 Installation Binary distributions of Doxygen for Linux x86-64, for Windows (Vista and later), and for macOS (10.14 and later) are available on the Doxygen Downloads page. To compile Doxygen from source, see the Doxygen Installation page. If you use a package manager, installation is quite easy. For example, running sudo apt-get install doxygen graphviz ghostscript on Ubuntu will get you not only Doxygen, but Ghostscript and the Graph visualization toolkit as well. (To do something similar on a Mac with Homebrew, run brew install doxygen graphviz ghostscript.) Depending on your system setup and operating system, you may additionally need to take pains to ensure that all required tools—gs, make, dot (from Graphviz), and doxygen itself—are on your system path. 2.3.4 Generating the Documentation If you have Make installed By far the easiest way to generate the complete Doxygen documentation with more or less the same customizations as appear in the on-line version is to open a terminal to the doxygen directory and run make all-html (Since all-html is the default Make target, you can also simply type make.) This will generate the most complete HTML version of the Doxygen documentation.8 To view it, open your browser to the landing page for the documentation, located at doxygen/doxygen_docs_complete/html/index.html. (On many systems, typing make view will generate the documentation and open it in a browser in a single step.) There are other Make targets available. To see a list, run make help These other Make targets may be useful if you want a PDF version of the documentation; or if you want to concentrate on the documentation of the BioCro modules only, or conversely, on only the C++ BioCro framework. If you don’t have Make installed Doxygen can be run directly by simply opening a terminal to the doxygen directory and typing doxygen on the command line. This produces almost the same results as running make. The differences are The browser won’t automatically open the documentation, as it does when you run make view. The About page won’t show information from Git about the state your working copy of the BioCro repository was in at the time you generated the documentation. The location of the generated documentation’s landing page will be doxygen/html/index.html rather than doxygen/doxygen_docs_complete/html/index.html. The canned Doxygen customization sets provided by Makefile are not available when you run doxygen directoy. 2.3.5 Speeding up Doxygen Method 1 By far, the most time consuming part of generating the Doxygen documentation is creating the graphs showing relationships between program components. You can speed up Doxygen tremendously by disabling the generation of these graphs. This is particularly recommended if you are tweaking the documentation of some source code and want to quickly see how your changes affect the rendered documentation (see the first use case in the “Use-Cases” section). To disable generation of graphs: Create a file called Doxyfile in the directory doxygen/customizations. Insert the single line HAVE_DOT = NO into this file. (If doxygen/customizations/Doxyfile already exists, simply add the line HAVE_DOT = NO to it.) Doing these two steps will override the setting HAVE_DOT = YES contained in doxygen/Doxyfile. (Bash shell users can disable graph generation for a single call of doxygen by running the command (cat Doxyfile; echo &quot;HAVE_DOT = NO&quot;) | doxygen - in the doxygen directory. If you use a different shell, see the question Can I configure doxygen from the command line? in the Doxygen FAQ.) Once you are satisfied with the way your Doxygen comments are rendered, you can, if you like, generate the documentation with the graphs by removing the HAVE_DOT = NO line from doxygen/customizations/Doxyfile.9 Method 2 Another way to speed up Doxygen is to run it only on the particular file whose documentation you are tweaking. For example, if you are adding to or changing the documentation in the source file src/module_library/partitioning_coefficient_logistic.h you can simply run (cat Doxyfile; echo &quot;INPUT=../src/module_library/partitioning_coefficient_logistic.h&quot;) | doxygen - in a Bash shell from the doxygen directory.10 This will run almost instantaneously! (The down side is that you will only be able to view the documentation pertaining to that particular file. You won’t, for example, be able to click on links to parent classes or dependent files and see the related documentation of those program entities.) 2.3.6 Doxygen for power users: Customizing Doxygen builds This section addresses Use Case 4 of the “Use-Cases” section. As we saw in the “Speeding Up Doxygen” section, we can customize Doxygen runs by adding settings to the file doxygen/customizations/Doxyfile. For example, adding HAVE_DOT = NO to this file will override the setting HAVE_DOT = YES contained in doxygen/Doxyfile. Doxygen offers dozens of settings that allow one to customize the generated documentation according to one’s preferences. We will highlight just a few of these settings that may be of special interest. For a complete list, see the Configuration section of the Doxygen documentation. INPUT We saw this setting in the “Speeding Up Doxygen” section, where we used it to override the default setting (which looks at all of the C++ source files) in order to concentrate on a particular file. This vastly reduced Doxygen’s running time. Note that multiple files (or directories!) may be listed here: simply separate the names with a space. Alternatively, list each on a separate line, using INPUT += on all but the first line. For example, INPUT = ../src/module_library/partitioning_coefficient_logistic.h INPUT += ../src/framework will yield documentation of the partitioning_coefficient_logistic module plus documentation of all of the source files in the src/framework directory. Note that the Make-file recipes ignore settings made to INPUT in the Doxyfiles. You will have to call doxygen directly for these settings to have any effect. Note also that Doxygen does not automatically recurse into subdirectories. For example, to also document the contents of src/framework/utils, you would need to add a third line: INPUT += ../src/framework/utils GENERATE_TREEVIEW The setting in doxygen/Doxyfile is YES. Since the Tree View index takes up screen width, setting this value to NO may facilitate easier browsing of source code. As with INPUT, the Make-file recipes normally override any setting made to this variable in the Doxyfiles. For this variable however, the value may be overridden on the command line using a call to Make of the form make generate_treeview=NO &lt;make target&gt; (see the note below). Or, simply call doxygen directly as in the INPUT case. HAVE_DOT As we saw in the previous section, setting this to NO will vastly reduce compilation time. (You should also set this to NO if you don’t have a copy of GraphViz, the package that contains the dot tool!) Unlike with INPUT and GENERATE_TREEVIEW, Make does respect the value this is set to in the Doxyfiles. For finer-grained control over which diagrams get generated, see the Doxygen documentation section Configuration options related to the dot tool. The doxygen/customizations directory contains a sample customization file named Doxyfile_customization_sample containing some suggestions for customizing your own Doxygen builds. To use this as a template, copy it to a file named Doxyfile (in the same directory) and modify it as you see fit. Note on customizing Make builds Certain customization variables are off limits when building the documentation using Make. These include the following: GENERATE_HTML, GENERATE_TREEVIEW, GENERATE_LATEX, INPUT, OUTPUT_DIRECTORY, EXTRACT_PRIVATE, and HTML_COLORSTYLE_HUE. If any of these tags are set in the customization file doxygen/customizations/Doxyfile, those settings will be ignored when using Make. This is because the Make file itself sets values for these variables, overriding any settings in the Doxyfiles. Three of these variables, however, can be set on the command line when running Make. This is done as follows: GENERATE_TREEVIEW To override the value YES that is set in Makefile, add a variable setting to the make command: make generate_treeview=NO &lt;make target&gt; HTML_COLORSTYLE_HUE To override the value 143 that is set in Makefile, add a variable setting to the make command. For example make color=30 &lt;make target&gt; For the meaning of the hue number, see the Wikipedia article on this subject.11 EXTRACT_PRIVATE To override the value YES that is set in Makefile``, add a variable setting to themake` command: make extract_private=NO &lt;make target&gt; This will produce documentation that focuses on the public API for classes, omitting documentation of private class members. 2.4 Building package vignettes 2.4.1 Required software Unless you only want to build vignettes written in Markdown (“.Rmd” files), you will need a TeX installation of some sort. Here are two options: Visit the CTAN starter page and choose and install a TeX distribution designed for your platform. Alternatively, if you mainly want a TeX installation for use in R, you can install the R tinytex package along with some extra needed LaTeX packages not included in TinyTeX by default by proceeding as follows: install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() # Install a few LaTeX packages needed by the vignettes but not # included in tinytex: tinytex::tlmgr_install(c(&#39;siunitx&#39;, &#39;babel-english&#39;, &#39;footnotebackref&#39;, &#39;listings&#39;, &#39;appendix&#39;, &#39;pgf&#39;)) (If you install TeX in this way, you will either need to build the vignettes using one of the Alternative options given below or add TinyTex’s bin directory to your shell path. You can find the root of the TinyTex installation with the R function tinytex::tinytex_root().) If you use the second alternative build option listed below, you will also need the R package devtools: install.packages(&#39;devtools&#39;) 2.4.2 Building all vignettes at once The simplest (but not fastest) way to build all the vignettes is to use the pkgdown package. From an R session running in the root directory of the BioCro source tree, type pkgdown::build_site(). This will build a local copy of the BioCro documentation web site, incuding all vignettes in the vignettes/web_only directory. (The bookdown book will not automatically be built, however.) This method doesn’t require that BioCro be installed. But it installs BioCro in a temporary location and then builds all the vignettes and help pages, and thus it can be somewhat time consuming. 2.4.3 Building individual vignettes From an R session running in biocro/vignettes, type tools::buildVignette(XXX), where XXX is the name of the particular vignette you wish to build. (It should have extension .Rnw or .Rmd.) The resulting PDF or HTML file will appear in biocro/vignettes. This method is relatively fast and so is especially useful if you are writing a new vignette or revising an existing one. It also enables vignettes in vignettes/web_only to be built. If the vignette being built uses any BioCro code, there must be a version of BioCro installed. 2.4.4 Alternative options Here are some alternative methods of building vignettes. The following instructions assume that the root of the BioCro source tree is a directory named biocro. Using R CMD build: Build the package by running R CMD build biocro from the command line in the directory containing biocro. This includes building the vignettes, but will exclude any in vignettes/web_only, because this directory has been added to BioCro’s .Rbuildignore file. Then install using R CMD INSTALL BioCro_xxx.tar.gz, where xxx is the version number. The vignettes should now be available as HTML or PDF files located in path_to_rlib/BioCro/doc, where path_to_rlib is the path to your R library directory. An easy way to pull up an index to the vignettes in a web browser is to run the command browseVignettes('BioCro') in an R session. Using devtools: From an R session running in any directory of the BioCro source tree, type devtools::build_vignettes(). (Alternatively, start R from anywhere and pass the path to BioCro source tree as the first (“pkg”) argument to build_vignettes().) This method will modify .Rbuildignore and .gitignore, which may be annoying. The resulting HTML and PDF files will appear in the doc directory, which will be created if it doesn’t already exist. This method doesn’t require that BioCro be installed. But it builds and installs BioCro in a temporary location, and thus it can be somewhat time consuming. Moreover, by default it gives very little indication of build progress, and so it may be useful to override this default and set quiet = FALSE in the function argument list. Note that devtools::build-vignettes() follows the same procedure as R CMD build, so it will not build any vignettes in vignettes/web_only. 2.5 Compiling and viewing the bookdown book Note: A copy of the bookdown BioCro developer’s manual is automatically generated on BioCro’s GitHub documentation site at https://biocro.github.io and may be viewed by clicking on the Developer’s Manual menu-bar link found there. What follows, therefore, will mostly be of interest to developers working on revising the documentation who want to check the results of their revisions. To generate the bookdown BioCro developer’s manual, do as follows: Install Pandoc, if it is not already on your system. See https://pandoc.org/installing.html for instructions. (Note to RStudio users: As mentioned in the R Markdown Cookbook (https://bookdown.org/yihui/rmarkdown-cookbook/install-pandoc.html), RStudio comes with its own copy of Pandoc, so you may be able to get by without installing it separately.) Install the R bookdown package, if it hasn’t been installed already. These instructions are written for bookdown version 0.22 or greater but may work for other versions. In the bookdown directory of your BioCro source tree, run Rscript -e &quot;bookdown::render_book()&quot; Note: If you wish to run render_book from other than the bookdown directory, you may pass a path argument: Rscript -e &quot;bookdown::render_book(&lt;path&gt;) Here, &lt;path&gt; denotes the path from the current directory to the bookdown directory. This only works in bookdown versions 0.22 and later! With earlier versions, you can make use of the xfun::in_dir function: xfun::in_dir(&#39;&lt;path&gt;&#39;, bookdown::render_book()) Again, &lt;path&gt; here denotes the path from the current directory to the bookdown directory. Note: Because some sections of the book are contained in their own files rather than being in a larger file comprising a complete chapter, render_book will issue a warning such as the following, which may be safely ignored: “In split_chapters(output, gitbook_page, number_sections, split_by, : You have 13 Rmd input file(s) but only 7 first-level heading(s). Did you forget first-level headings in certain Rmd files?” In a Web browser, open bookdown/_book/index.html. Ghostscript is used to convert the PostScript files that are generated for formulas in the documentation into bitmaps. But MathJax provides an alternative method of rendering formulas in the HTML documentation, and so Ghostscript is unneeded as long as the Doxygen configuration variable USE_MATHJAX is set to YES. This is the BioCro default setting. If Ghostscript is used, there may be some compatibility issues between Ghostscript and Doxygen. If you encounter problems, see Doxygen issue #7290 and Doxygen issue #8107 for further information. On the other hand, use of MathJax requires a working internet connection, at least until your browser downloads and caches the MathJax code file. And it requires that JavaScript be enabled.↩︎ Although versions earlier than 4.3 will probably mostly work, you will get a warning when you use them.↩︎ Some users will have to type gmake in place of make to get the latest version of Gnu Make. Type make --version or gmake --version to see exactly what version you are calling.↩︎ If you are compiling the documentation using Make and haven’t changed any source files since last running it, add the -B option to force recompilation.↩︎ Alternatively, if you will be working extensively with one particular file, you could temporarily add INPUT=../src/module_library/partitioning_coefficient_logistic.h to the customization file (doxygen/customizations/Doxyfile). Then you would simply run doxygen from the doxygen directory. Note that even though the customization file is in the doxygen/customizations directory, the file paths listed in the INPUT value should be relative to the doxygen directory!↩︎ Other settings that affect the color of the documentation are HTML_COLORSTYLE, HTML_COLORSTYLE_SAT, and HTML_COLORSTYLE_GAMMA. The values for these options must be set in the customization file doxygen/customizations/Doxyfile if using Make; alternately, if calling doxygen directly, they may be set on the command line; for example (cat Doxyfile; echo &quot;HTML_COLORSTYLE_SAT=255&quot;) | doxygen - See the Configuration section of the Doxygen documentation for information about these settings.↩︎ "],["running-testthat.html", "3 Running the testthat Tests 3.1 Requirements 3.2 tl;dr 3.3 tl;dr for devtools users 3.4 Continuous integration workflow; why run tests manually? 3.5 Test-running scenarios 3.6 Running individual test files 3.7 Using devtools", " 3 Running the testthat Tests 3.1 Requirements The testthat package. From the R command line, run install.packages('testthat'). 3.2 tl;dr To run all testthat tests, move to the tests directory within the BioCro source code directory tree and run Rscript testthat.R Warning: This assumes the BioCro package has been installed. It will run all tests in the tests/testthat directory against this installed version. If you want the tests to reflect changes to your source code, reinstall BioCro before running them, or use one of the alternative test-running methods outlined below. 3.3 tl;dr for devtools users If you use the devtools package, you can run all tests by doing the following command in any directory in the BioCro source tree: Rscript -e &quot;devtools::test()&quot; This will test against the source code; it does not expect BioCro to be installed. Note that this source code test will compile or recompile the C++ code if necessary. See the “Source-Code Testing” section for details. 3.4 Continuous integration workflow; why run tests manually? BioCro’s testthat test suite is automatically run on GitHub as part of the R-CMD-check workflow every time a BioCro developer makes or modifies a pull request. Users also have the option to run this workflow manually by clicking a button on the GitHub page for R-CMD-check workflow runs. All R-CMD-check workflow run results are viewable here as well. There are (at least) two scenarios, however, under which you may want to run tests manually: You have revised the package’s R code, stored data, or C++ code, and you want to run the testthat test suite against the changed code on your own machine before pushing that code to GitHub. You are writing new tests, and you want to ensure that they work as expected. Read on for further information about various topics, including testing package source versus testing an installed package better test reporting options running individual test files 3.5 Test-running scenarios There are two main ways to run the BioCro testthat tests: Run tests in the tests/testthat directory of a BioCro source tree against an installed version of BioCro Run tests in the tests/testthat directory of a BioCro source tree against the code in that source tree (A third scenario exists: running installed testthat tests against the installed package. But currently the BioCro package does not install any of its tests.) 3.5.1 Running the test suite on the installed version of BioCro As explained in the “TL;DR” section above, the easiest way to run the tests against an installed version of the BioCro package is to move to the tests directory within the BioCro source code directory tree and run Rscript testthat.R Alternatively, the tests may be run inside an R session as follows: Start an R session (if you don’t have one open already). Run xfun::in_dir('&lt;path to tests directory&gt;', source('testthat.R')), where &lt;path to tests directory&gt; is the path to the tests directory of a BioCro source tree. OR Use setwd to move to the tests directory (if you aren’t there already) and just run source('testthat.R'). Either of these methods will run the testthat tests in the same way that R CMD check would run them. (But R CMD check builds BioCro immediately before running the tests, ensuring that you are running the tests against a version of the BioCro package corresponding to the BioCro code in your source tree.) 3.5.2 Switching reporters The default output of the test method used by testthat.R can be exceedingly terse, and so it is highly desirable to tweak the test output of that method for interactive use. This is done by overriding the default reporter of the testing function using the reporter argument. A particularly useful and informative reporter is the Summary reporter. To use it, run the following commands in an R session: library(BioCro) library(testthat) xfun::in_dir(&#39;&lt;path to tests directory&gt;&#39;, test_check(&#39;BioCro&#39;, reporter = &#39;Summary&#39;)) (If you are in the tests directory, the third line can be simply test_check(&#39;BioCro&#39;, reporter = &#39;Summary&#39;) And if you have already loaded the BioCro and testthat packages (either directly or by sourcing testthat.R), there is no need to reload them!) The Summary reporter is useful in two ways: It clearly indicates testing progress by printing a character to the screen each time a test completes (“.” for success, “S” for a skipped test, “W” for a warning, and an (error) number for a failed test). And it prints the file name of each test file before printing that file’s context message. Setting reporter to ‘Progress’ yields slightly less verbose output. Crucially and inconveniently, it doesn’t print the names of the test files being run, although it does print their context messages. On the other hand, it gives a better numerical summary of the test results—how many tests passed, how many failed, and how many were skipped.12 3.5.3 Running the test suite against the BioCro source code If you are making changes to the R code or to the C++ code (or even to the package data) and you want to test your changes to the source code without installing (or re-installing) the BioCro package, the function to use is test_local.13 The steps are similar to the steps above for running test_check: Start an R session. Load the testthat library: library(testthat). (Note that we don’t need to load the BioCro library, and there needn’t even be a copy of it installed!) Run test_local('&lt;path to the tests directory of some BioCro source tree&gt;') If you are actually in the tests directory—either because you started R there or because you moved there with setwd—you can just run test_local(), because the path defaults to '.'. Note that this function expects to find an up-to-date copy of the BioCro C++ library file (BioCro.so, or BioCro.dll on Windows) in the src directory. If it doesn’t find it (or if it is out of date with respect to the C++ source files), it will try to re-create it. (This will happen even if none of the tests use any of the package code.) So be patient if the function seems to hang for several minutes while it does this! The default reporter for test_local is the Progress reporter, but if you prefer the Summary reporter, which gives better progress indication and prints the name of each test file it runs, you can switch: test_local(&#39;&lt;path to the tests directory of some BioCro source tree&gt;&#39;, reporter = &#39;Summary&#39;)` 3.6 Running individual test files While writing new tests, it is useful to be able to run a single test file rather than the whole test suite. The test may be run either against the installed version of the BioCro package, or against the source code. 3.6.1 Running an individual test file on the installed package Method 1 Start an R session. Load the BioCro and testthat packages: library(BioCro) library(testthat) Call the test_file function on the path to a test file: test_file(&#39;&lt;path to test file to run&gt;&#39;) Note the path passed to test_file can be either a relative or an absolute path. It doesn’t matter what directory the R session was started in or what the current R directory is as long as the path is correct. Making the testthat directory the current directory, however, will make for shorter path names. Once again, the default reporter (“CompactProgress”, in this case) may be overridden: test_file(&#39;&lt;path to test file to run&gt;&#39;, reporter = &#39;Summary&#39;) See the documentation for testthat::Reporter for a list of reporters. Method 2 This uses the test_check function we used earlier (see the “Switching Reporters” section, but with a “filter” option. Start an R session. Load the BioCro and testthat packages: library(BioCro) library(testthat) Call the test_check function with a filter option to select the desired test file. The filter pattern should be a regular expression matching the main part of the test file. For example, to run the tests in test.HarmonicOscillationModeling.R, we could setwd to the tests directory and call test_check as follows: test_check(&#39;BioCro&#39;, filter = &#39;Harm&#39;) The filter matching is performed on file names after they are stripped of “test-” and “.R”. Again, a reporter option may be specified. For example, test_check(&#39;BioCro&#39;, filter = &#39;Harm&#39;, reporter = &#39;Summary&#39;) Note that step 3 assumes you are in the BioCro tests directory when test_check is called. If you aren’t, either usesetwd get there first, or use the xfun::in_dir wrapper: xfun::in_dir(&#39;&lt;path to tests directory&gt;&#39;, test_check(&#39;BioCro&#39;, filter = &#39;Harm&#39;)) 3.6.2 Running an individual test file against the package source code Again, the test_local function is used. The method is exactly the same as specified above in the “Testing Local” section except that a filter option is used to limit testing to matching files (see Method 2 in the previous section). 3.7 Using devtools If you don’t mind installing and using the devtools package, it provides a particularly easy way to run tests against the package source code: simply issue the following command in any directory in the BioCro source tree: Rscript -e &quot;devtools::test()&quot; Again, the filter option may be used with this function to limit the tests run, and the default reporter may be overridden with the reporter option. The names “Progress” and “Summary” almost seem to me as if they have been reversed! After all, it is the Summary reporter which most clearly indicates how the testing is progressing and the Progress reporter that gives the best numerical summary of how many tests failed.↩︎ Or you can use devtools::test(). See tl;dr for devtools users and Using devtools.↩︎ "],["r-cmd-check.html", "4 Running R CMD check 4.1 Running R CMD check online 4.2 Running R CMD check locally", " 4 Running R CMD check R includes a command-line tool called R CMD check whose purpose is to check whether an R package meets key requirements. At minimum, any package submitted to CRAN must pass R CMD check without producing any errors. R CMD check includes the regression tests discussed in the “Running the testthat Tests” sectionbut is also more expansive. Key components of R CMD check are: It runs the regression tests in the tests directory. For more information about using the testthat package for regression tests, see the Testing section of the “R Packages” textbook. It runs all of the examples, which are short pieces of code in the documentation for individual functions in the package. The examples can be found in the examples sections of the .Rd files in the man directory. For more information about examples, see the Examples section of the “R Packages” textbook. It builds the vignettes, which are long-form pieces of documentation that can be found in the vignettes directory. For more information about vignettes, see the Vignettes section of the “R Packages” textbook. Note that any vignettes inside the vignettes/web_only directory are excluded from this check because that directory has been added to BioCro’s .Rbuildignore file. For more information about the individual checks that compose R CMD check, please see its entry in the “R Packages” textbook. In the course of BioCro development, we frequently run R CMD check to make sure our code meets these requirements. Since R CMD check is so thorough, it is an important tool for identifying issues that may occur when changes are made to the code. R CMD check identifies issues with a range of severity. Anything labeled as an “error” is a serious issue and must be addressed. “Warnings” should almost always be addressed, and “notes” are the least serious. There are several ways to run R CMD check, as described below. 4.1 Running R CMD check online There is a GitHub action for running R CMD check on multiple operating systems and versions of R. It runs automatically in several situations (such as creating a pull request), but also can be triggered manually on any branch. See the “Continuous integration workflow” section for more details. Running R CMD check online tends to be slower than running it locally. 4.2 Running R CMD check locally 4.2.1 From the command line Since R CMD check is a command line tool, a basic way to run it is from the command line. It is necessary to run R CMD build first; this “builds” the package by building the vignettes and compiling all the files into a single archive. To do this, open a terminal running in the parent directory of a local copy of the BioCro repository, and then run the following command, assuming that the BioCro repository is stored in a directory called biocro: R CMD build --compact-vignettes=both biocro If there are any issues with the vignettes, one or more errors will occur at this stage. They must be addressed to actually “build” the package. If this command is successful, it will create a new file called BioCro_X.Y.Z.tar.gz, where X.Y.Z is the version of the BioCro package that was built. Then the package can be checked with R CMD check --as-cran BioCro_X.Y.Z.tar.gz In these commands, --compact-vignettes=both and --as-cran are optional arguments. We usually compact the vignettes before submitting to CRAN to ensure we meet package size requirements, while the “as-cran” option ensures that the R CMD check behavior matches the checks actually used by CRAN. 4.2.2 From within R There are also options for running R CMD check from within R. These R functions generally build and check the package, so they can be simpler than the command line approach. The central option is the rcmdcheck function from the rcmdcheck package. To use it, start an R session running in the parent directory of a local copy of the BioCro repository, and then run the following command, assuming that the BioCro repository is stored in a directory called biocro: rcmdcheck::rcmdcheck(path = &#39;biocro&#39;, args = &#39;--as-cran&#39;, error_on = &#39;warning&#39;) Here the “as-cran” and “error on warning” options ensure that the R CMD check behavior matches the checks actually used by CRAN. It also possible to use the check function from the devtools package, although that command is essentially just a wrapper for rcmdcheck::rcmdcheck. "],["adding-the-boost-libraries.html", "5 Adding the Boost libraries 5.1 Why this is needed 5.2 How to extract parts of Boost", " 5 Adding the Boost libraries 5.1 Why this is needed BioCro uses software from the Boost C++ libraries. Boost does not assure backward compatibility, so changes to Boost could break BioCro. Thus, we don’t want to link our code to a user supplied Boost installation, and we include a version with BioCro. Boost is very large, so we want to include only the necessary parts. This document lists steps to extract the relevant parts and update files in BioCro to use them. 5.2 How to extract parts of Boost Use the bcp tool included with Boost to extract parts of the Boost library. bcp accepts a list of files or modules and extracts the relevant parts of the Boost library to a directory. The boost files that BioCro uses are as follows: File name Notes boost/config.hpp Used in module_dependency_utilities.cpp boost/graph/adjacency_list.hpp Used in module_dependency_utilities.cpp boost/graph/topological_sort.hpp Used in module_dependency_utilities.cpp boost/math/constants/constants.hpp Used in constants.h boost/numeric/odeint.hpp Used in ode_solver.h boost/numeric/ublas/io.hpp Used in newton_raphson_boost.h boost/numeric/ublas/lu.hpp Used in newton_raphson_boost.h boost/numeric/ublas/matrix.hpp Used in newton_raphson_boost.h boost/numeric/ublas/triangular.hpp Used in newton_raphson_boost.h boost/numeric/ublas/vector.hpp Used in boost_ode_solvers.h boost/numeric/ublas/vector_proxy.hpp Used in newton_raphson_boost.h boost/typeof/incr_registration_group.hpp This is needed for boost/units but is not exported properly boost/graph/detail/empty_header.hpp This is needed for topological_sort but is not exported properly Note that we do not actually use boost/units, but it is included whenever boost/numeric/odeint.hpp is included, and BioCro will not compile on some operating systems without it. Hence the need for boost/typeof/incr_registration_group. Run the following command, noting that the path to the temporary directory must exist: bcp --boost=\"PATH_TO_BOOST_ROOT_DIRECTORY\" \"boost/config.hpp\" \"boost/graph/adjacency_list.hpp\" \"boost/graph/topological_sort.hpp\" \"boost/math/constants/constants.hpp\" \"boost/numeric/odeint.hpp\" \"boost/numeric/ublas/io.hpp\" \"boost/numeric/ublas/lu.hpp\" \"boost/numeric/ublas/matrix.hpp\" \"boost/numeric/ublas/triangular.hpp\" \"boost/numeric/ublas/vector.hpp\" \"boost/numeric/ublas/vector_proxy.hpp\" \"boost/typeof/incr_registration_group.hpp\" \"boost/graph/detail/empty_header.hpp\" PATH_TO_TEMPORARY_DIRECTORY Copy PATH_TO_TEMPORARY_DIRECTORY/boost to the inc directory, overwriting any previous version of boost. Other files and directories my be created in PATH_TO_TEMPORARY_DIRECTORY, but they are not needed. Check that the Boost license in inc is correct for the version used, and update the package LICENSE file if necessary. Update the path to the Boost license in the package LICENSE file. Run R CMD check and address any new warnings or errors related to the boost library. It is likely that the following issues will occur: Some file paths will exceed the 100 character limit. Truncate any such file names, and be sure to update any associated #include directives that reference these files; otherwise, compilation errors will occur. As of boost version 1.83, some of the boost libraries included with BioCro call sprintf, which is not allowed by CRAN. Replace any such instances of sprintf with snprintf. 5.2.1 Notes for using bcp in Windows First, follow the instructions in the “Getting Started on Windows” Boost page. For V1.71.0, this entails the following: - Install the Visual Studio 2019 Developer Command Prompt (see here). Download the full Boost library, unzip it, and put it somewhere convenient: e.g., C:\\Program Files\\boost\\boost_1_71_0. Open a VS developer prompt and cd into the boost root directory: e.g., cd C:\\Program Files\\boost\\boost_1_71_0. Type bootstrap and press enter. This may take a minute or so to complete. Type .\\b2 and press enter. This may also take a little while. Now the Boost libraries have been built, and we are almost ready to use bcp. However, we need to explicitly build the bcp tool using the Boost.Build tool, which is cryptically named b2.exe. In a VS developer prompt, cd into the tools/bcp directory: e.g., cd C:\\Program Files\\boost\\boost_1_71_0\\tools\\bcp. Run b2.exe using its full path: e.g., type C:\\\"Program Files\"\\boost\\boost_1_71_0\\b2 and press enter. Now bcp.exe should be in C:\\Program Files\\boost\\boost_1_71_0\\dist\\bin. Finally, cd into the folder that contains bcp: e.g., cd C:\\Program Files\\boost\\boost_1_71_0\\dist\\bin. Now you should be able to run the command listed as “Step 1” above from a VS developer command prompt. "],["the-generate-documentation-workflow.html", "6 The Generate Documentation Workflow", " 6 The Generate Documentation Workflow The Generate Documentation workflow, contained in file document.yml, builds the BioCro User’s Manual, the BioCro Developer’s Manual, and the Doxygen BioCro C++ Library documentation and copies and commits the result to the repository given as the value of the “PUBLISH_TO” environment variable (henceforth the documentation repository; see the value contained in document.yml). That repository is in turn set up to publish to a GitHub Pages Web site at the corresponding canonical location. (In general, a repository named &lt;organization&gt;/&lt;repository&gt; will be published to the URL https://&lt;organization&gt;.github.io/&lt;repository&gt;.) The workflow runs whenever a new commit is pushed to the main branch; whenever a new version tag is applied; whenever a new pull request is made or altered. (In the latter case, the documentation is not published but is made available as an artifact for download.) It may also be run manually on the GitHub site. In order for this to all work correctly, a number of set-up steps were required. Here are the steps involved: An SSH public/private key pair is generated on a work station using the command ssh-keygen -t rsa -b 4096. Use the -C option to include a comment; for example ssh-keygen -t rsa -b 4096 \\ -C &quot;Let biocro/biocro deploy to biocro/BioCro-documentation&quot; You will be prompted for a file name for the key, and then for a pass-phrase. The pass-phrase should be empty. This will generate a pair of files, one with the public key and one with the private key. For example, if you choose biocro-key as the name of the file, this will be the name of the file containing the private key, and the public key will be in a file called biocro-key.pub. This key pair is needed in order to allow a workflow defined in the BioCro repository to push files to the documentation repository. The private key must be added as an Actions secret in the BioCro repository (this repository) under the name PRIVATE_SSH_KEY. When doing this, paste the entire contents of the private key file into text area labeled Secret. (See https://docs.github.com/en/actions/reference/encrypted-secrets#creating-encrypted-secrets-for-a-repository. The key name matches the reference secrets.PRIVATE_SSH_KEY used in the document.yml workflow file.) Create the documentation repository. Even though the Generate Documentation workflow will populate this repository, at least one file must be added to it to begin with in order for the workflow to run. (Suggestion: Add a README.md file when creating the repository; it can be blank.) Add the public SSH key as a deploy key to the documentation repository under a suitable title; for example, “Access from biocro/biocro documentation workflow” (assuming “biocro/biocro” is the name of this repository). When doing this, paste the entire contents of the public key file into the text area labeled Key. (See https://docs.github.com/en/developers/overview/managing-deploy-keys#setup-2. The title “Access from biocro/biocro documentation workflow” is for informational purposes only and has no programmatic significance.) Enable GitHub Pages for the documentation repository (see https://pages.github.com/). This results in the files in the documentation repository getting automatically published to the corresponding GitHub Pages web site. "],["the-doxygen-docker-action-using-makefile-action.html", "7 The Doxygen Docker Action using Makefile Action 7.1 Inputs", " 7 The Doxygen Docker Action using Makefile Action This action is a customization of the code found at https://github.com/mattnotmitt/doxygen-action. Instead of calling Doxygen directly, it assumes that there is a Make file in the working directory that invokes Doxygen to build the documentation. 7.1 Inputs 7.1.1 ‘working-directory’ Required Path of the working directory to change to before running Make. This should be the location of the Make file used with Doxygen. 7.1.2 ‘color’ Optional The color setting (given as an angle from 0 to 360 in the HSL/HSV color space). Default: 143 (greenish) 7.1.3 ‘document-private’ Optional YES to document private class members, NO to document only the public and protected ones. Default: YES 7.1.4 ‘generate-treeview’ Optional YES to generate the Tree View index, NO to omit it. Since the Tree View index takes up screen width, omitting it may facilitate easier browsing of source code. Default: YES 7.1.5 ‘extra-settings’ Optional A space-separated set of settings of the form key=value. Only keys recognized by the make file will have any effect. Example: module_docs_directory=doxygen_docs_modules_public_members_only will change the output directory for module-docs-* targets from doxygen_docs_modules to doxygen_docs_modules_public_members_only. Default: ’’ (blank) 7.1.6 ‘makefile-target’ Optional The name of the Make target to use. Default: ’’ (none) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
